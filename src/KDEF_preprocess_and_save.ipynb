{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Augments and saves KDEF Data to Preprocessed Folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running this notebook will:\n",
    "- firstly create a pd dataframe with emotion labels and img paths for KDEF data\n",
    "- augment and save to training/validation/test folders with an equal number of samples per emotion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE: KDEF basically has the same number of images per class, so you can just run this notebook to directly split and save the data into the Preprocessed data folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset download instructions - do this before running notebook\n",
    "\n",
    "- KDEF dataset: https://www.kdef.se/?fbclid=IwAR102R1eWOMWp87LQK83DDGRsNVLvofz1DdV6TtCGl5tFivNmo3KzEbJc84\n",
    "Download 'KDEF_and_AKDEF' from above link, and put it under '../FER_Resnet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import os, os.path\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy.ndimage\n",
    "from PIL import Image\n",
    "import preprocess as p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manually delete \"black images\"\n",
    "- Note: black images are supposed to have all 0 entries, but this doesn't seem to be the case with our images\n",
    "- We can manually delete all the iamges that are \"black\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully deleted 0 black images\n"
     ]
    }
   ],
   "source": [
    "p.delete_black_images_KDEF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dataframe of KDEF images. Columns are emotion labels and img paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    420\n",
       "2    420\n",
       "0    420\n",
       "5    419\n",
       "6    419\n",
       "4    419\n",
       "1    418\n",
       "Name: emotion, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KDEF_df = p.get_KDEF_df(sideview=False, halfside=True, straight=True)\n",
    "KDEF_df[\"emotion\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>img_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>../datasets/KDEF_and_AKDEF/KDEF/AF01/AF01AFHL.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>../datasets/KDEF_and_AKDEF/KDEF/AF01/AF01AFHR.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>../datasets/KDEF_and_AKDEF/KDEF/AF01/AF01AFS.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>../datasets/KDEF_and_AKDEF/KDEF/AF01/AF01ANHL.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>../datasets/KDEF_and_AKDEF/KDEF/AF01/AF01ANHR.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4884</th>\n",
       "      <td>4</td>\n",
       "      <td>../datasets/KDEF_and_AKDEF/KDEF/BM35/BM35SAHR.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4885</th>\n",
       "      <td>4</td>\n",
       "      <td>../datasets/KDEF_and_AKDEF/KDEF/BM35/BM35SAS.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4888</th>\n",
       "      <td>5</td>\n",
       "      <td>../datasets/KDEF_and_AKDEF/KDEF/BM35/BM35SUHL.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4889</th>\n",
       "      <td>5</td>\n",
       "      <td>../datasets/KDEF_and_AKDEF/KDEF/BM35/BM35SUHR.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4890</th>\n",
       "      <td>5</td>\n",
       "      <td>../datasets/KDEF_and_AKDEF/KDEF/BM35/BM35SUS.JPG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2935 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      emotion                                           img_path\n",
       "2           2  ../datasets/KDEF_and_AKDEF/KDEF/AF01/AF01AFHL.JPG\n",
       "3           2  ../datasets/KDEF_and_AKDEF/KDEF/AF01/AF01AFHR.JPG\n",
       "4           2   ../datasets/KDEF_and_AKDEF/KDEF/AF01/AF01AFS.JPG\n",
       "7           0  ../datasets/KDEF_and_AKDEF/KDEF/AF01/AF01ANHL.JPG\n",
       "8           0  ../datasets/KDEF_and_AKDEF/KDEF/AF01/AF01ANHR.JPG\n",
       "...       ...                                                ...\n",
       "4884        4  ../datasets/KDEF_and_AKDEF/KDEF/BM35/BM35SAHR.JPG\n",
       "4885        4   ../datasets/KDEF_and_AKDEF/KDEF/BM35/BM35SAS.JPG\n",
       "4888        5  ../datasets/KDEF_and_AKDEF/KDEF/BM35/BM35SUHL.JPG\n",
       "4889        5  ../datasets/KDEF_and_AKDEF/KDEF/BM35/BM35SUHR.JPG\n",
       "4890        5   ../datasets/KDEF_and_AKDEF/KDEF/BM35/BM35SUS.JPG\n",
       "\n",
       "[2935 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KDEF_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augment KDEF images and save to train/validate/test folders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Reads and resizes an image from the mux dataset to 48 x 48\n",
    "Also makes it grayscale\n",
    "'''\n",
    "\n",
    "def read_and_resize(img_path, plot_img):   \n",
    "    \"\"\"\n",
    "    Args:\n",
    "        img_path: relative path to the KDEF image\n",
    "        \n",
    "    Returns: the resized image as numpy array\n",
    "    \"\"\" \n",
    "    # Read and resize image with OpenCV \n",
    "    img_pixels = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2GRAY) # cv2 reads in BGR, need to convert to grayscale\n",
    "\n",
    "    # Perform a crop so that the image is square i.e. 560x560\n",
    "    crop_img = np.asarray(img_pixels[100:660, 0:560])\n",
    "    \n",
    "    # Resize to 48 x 48\n",
    "    crop_img = cv2.resize(crop_img, dsize=(48, 48), interpolation=cv2.INTER_CUBIC)\n",
    "    img_data = np.asarray(crop_img)\n",
    "    \n",
    "    # Plot if plot_img = True\n",
    "    #if plot_img == True:\n",
    "        #plt.imshow(crop_img)\n",
    "        #cv2.imshow(\"Converted Image\",crop_img)\n",
    "        #print(\"img array shape:\",crop_img.shape)\n",
    "        #cv2.waitKey(0)\n",
    "           \n",
    "    return img_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.4.0) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-9d_dfo3_\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-773f32069872>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mread_and_resize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"./KDEF_and_AKDEF/KDEF/AF01/AF01AFFR.JPG\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-c01ed90fc8b5>\u001b[0m in \u001b[0;36mread_and_resize\u001b[1;34m(img_path, plot_img)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \"\"\" \n\u001b[0;32m     13\u001b[0m     \u001b[1;31m# Read and resize image with OpenCV\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mimg_pixels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# cv2 reads in BGR, need to convert to grayscale\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m# Perform a crop so that the image is square i.e. 560x560\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.4.0) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-9d_dfo3_\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"
     ]
    }
   ],
   "source": [
    "read_and_resize(img_path=\"./KDEF_and_AKDEF/KDEF/AF01/AF01AFFR.JPG\", plot_img = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Returns an array of images as 3 pytorch tensors: [Original, Flipped, Rotated by 5 degrees]\n",
    "'''\n",
    "\n",
    "def augument(img):\n",
    "    \n",
    "    # Original Image and Add RGB channels\n",
    "    img_tensor = torch.from_numpy(np.copy(img)).unsqueeze(0).repeat(3,1,1)\n",
    "    # Rotate Image and add RGB channels\n",
    "    img_rotated_tensor = torch.from_numpy(scipy.ndimage.rotate(np.copy(img), 5, order=1, reshape=False)).unsqueeze(0).repeat(3,1,1)\n",
    "    # Flip image and add RGB channels\n",
    "    img_flipped_tensor = (torch.from_numpy(np.fliplr(np.copy(img)).copy())).repeat(3,1,1)\n",
    "    \n",
    "    return [img_tensor, img_rotated_tensor, img_flipped_tensor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_save(df, total_images):\n",
    "    \n",
    "    \"\"\"\n",
    "    Preprocesses and saves KDEF data to the preprocessed data directory. Splits into train/validate/test and ensures equal \n",
    "    number of samples per class are saved for the KDEF dataset.\n",
    "    \n",
    "    Images are saved as \"KDEF\" + <integer> + <augmentation>\n",
    "\n",
    "    Args: \n",
    "        KDEF dataframe created above\n",
    "        total_images = number of images per class\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Paths to save processed tensor\n",
    "    train_path = './ProcessedNoCutoffData/train/' \n",
    "    val_path = './ProcessedNoCutoffData/validate/' \n",
    "    test_path = './ProcessedNoCutoffData/test/' \n",
    "    \n",
    "    # Check if the save paths exist, make them if they don't\n",
    "    if not os.path.isdir(train_path):\n",
    "        os.mkdir(train_path)\n",
    "    if not os.path.isdir(val_path):\n",
    "        os.mkdir(val_path)\n",
    "    if not os.path.isdir(test_path):\n",
    "        os.mkdir(test_path)\n",
    "        \n",
    "    # Decides when to save to which folder\n",
    "    train_count = (total_images * 0.7)               \n",
    "    val_count = train_count + total_images * 0.15    \n",
    "    test_count = total_images\n",
    "    \n",
    "    # Current count - counts how many images in each class have been saved currently\n",
    "    # First 70% saves to train_path, next 15% saves to val_path, next 15% saves to test path\n",
    "    count = [0, 0, 0, 0, 0, 0, 0]\n",
    "    \n",
    "    num_imgs = len(df.index)\n",
    "    \n",
    "    for i in range(num_imgs):\n",
    "        \n",
    "        # retrieve img path \n",
    "        img_path = df.iloc[i][\"img_path\"]    \n",
    "        \n",
    "        # crop/resize to 48x48\n",
    "        img_data = read_and_resize(img_path,plot_img=False)\n",
    "        \n",
    "        # Normalize to between 0 and 1\n",
    "        img_data = img_data/255   # Since values are in (0, 255)\n",
    "        \n",
    "        # Augument: Add RGB, Flip, Rotate\n",
    "        # Returns 3 tensors\n",
    "        augumented_images = augument(img_data)\n",
    "        emotion = df.iloc[i][\"emotion\"]\n",
    "        \n",
    "        # Decide whether to save to train, val, or test\n",
    "        if(count[int(emotion)] < train_count):\n",
    "            folder_name = train_path + str(emotion)\n",
    "        elif(count[int(emotion)] < val_count):\n",
    "            folder_name = val_path + str(emotion)\n",
    "        elif(count[int(emotion)] < test_count):\n",
    "            folder_name = test_path + str(emotion)\n",
    "        \n",
    "        # Save if total images for emotion is less than total images\n",
    "        if(count[int(emotion)] < total_images):\n",
    "            \n",
    "            # Create directory for emotion if it doesn't already exist\n",
    "            if not os.path.isdir(folder_name):\n",
    "                os.mkdir(folder_name)\n",
    "\n",
    "            # Save original and augmented images\n",
    "            torch.save(augumented_images[0], folder_name + '/KDEF' + str(i) + '.tensor')\n",
    "            torch.save(augumented_images[1], folder_name + '/KDEF' + str(i) + '_rotated.tensor')\n",
    "            torch.save(augumented_images[2], folder_name + '/KDEF' + str(i) + '_flipped.tensor')\n",
    "            count[int(emotion)] += 3\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_and_save(df=KDEF_df, total_images=1257)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
